{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Dense neural network with PyTorch\n",
    "Authors: Javier Duarte, Tyler Mitchell, Raghav Kansal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to download the data if you did not already download it in from Tutorial #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget -O data/ntuple_4mu_bkg.root https://zenodo.org/record/3901869/files/ntuple_4mu_bkg.root?download=1\n",
    "!wget -O data/ntuple_4mu_VV.root https://zenodo.org/record/3901869/files/ntuple_4mu_VV.root?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `pandas` DataFrames\n",
    "Now we load two different `NumPy` arrays. One corresponding to the VV signal and one corresponding to the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy TTree HZZ4LeptonsAnalysisReduced into a pandas DataFrame\n",
    "treename = 'HZZ4LeptonsAnalysisReduced'\n",
    "filename = {}\n",
    "upfile = {}\n",
    "df = {}\n",
    "\n",
    "filename['VV'] = 'data/ntuple_4mu_VV.root'\n",
    "filename['bkg'] = 'data/ntuple_4mu_bkg.root'\n",
    "\n",
    "# Drop all variables except for those we want to use when training.\n",
    "VARS = ['f_mass4l','f_massjj']\n",
    "\n",
    "upfile['VV'] = uproot.open(filename['VV'])\n",
    "upfile['bkg'] = uproot.open(filename['bkg'])\n",
    "\n",
    "df['bkg'] = upfile['bkg'][treename].arrays(VARS, library='pd')\n",
    "df['VV'] = upfile['VV'][treename].arrays(VARS, library='pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inputs are well behaved.\n",
    "df['VV']= df['VV'][(df['VV'][VARS[0]] > -999) & (df['VV'][VARS[1]] > -999)]\n",
    "df['bkg']= df['bkg'][(df['bkg'][VARS[0]] > -999) & (df['bkg'][VARS[1]] > -999)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add isSignal variable\n",
    "df['VV']['isSignal'] = np.ones(len(df['VV'])) \n",
    "df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into testing and training dataset\n",
    "\n",
    "We will split the data into two parts (one for training+validation and one for testing). \n",
    "We will also apply \"standard scaling\" preprocessing: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html i.e. making the mean = 0 and the RMS = 1 for all input variables (based **only** on the training/validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine signal and background into one DataFrame then split into input variables and labels.\n",
    "NDIM = len(VARS)\n",
    "df_all = pd.concat([df['VV'],df['bkg']])\n",
    "dataset = df_all.values\n",
    "X = dataset[:,0:NDIM]\n",
    "Y = dataset[:,NDIM]\n",
    "\n",
    "# Split into training and testing data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "print(X_train_val)\n",
    "print(X)\n",
    "\n",
    "# preprocessing: standard scalar (reshape inputs to mean=0, variance=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split again, this time into training and validation data.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "We'll start with a dense (fully-connected) NN layer.\n",
    "Our model will have a single fully-connected hidden layer with the same number of neurons as input variables. \n",
    "The weights are initialized using a small Gaussian random number. \n",
    "We will switch between linear and tanh activation functions for the hidden layer.\n",
    "The output layer contains a single neuron in order to make predictions. \n",
    "It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1.\n",
    "\n",
    "We are using the `binary_crossentropy` loss function during training, a standard loss function for binary classification problems. \n",
    "We will optimize the model with the Adam algorithm for stochastic gradient descent and we will collect accuracy metrics while the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our model. \n",
    "import torch\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Use Binary Cross Entropy as our loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Optimize the model parameters using the Adam optimizer.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data ready\n",
    "val_data = torch.from_numpy(X_val).float()\n",
    "val_label = torch.from_numpy(Y_val).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training \n",
    "Here, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for progress bar while training\n",
    "\n",
    "losses, val_losses = [], []\n",
    "min_loss, stale_epochs = 100., 0\n",
    "\n",
    "# 500 epochs. \n",
    "batch_size = 1024\n",
    "for t in tqdm(range(500)):\n",
    "    batch_loss, val_batch_loss = [], []\n",
    "    \n",
    "    for b in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[b:b+batch_size]\n",
    "        Y_batch = Y_train[b:b+batch_size]\n",
    "        x = torch.from_numpy(X_batch).float()\n",
    "        y_b = torch.from_numpy(Y_batch).float()\n",
    "        y_b = y_b.view(-1, 1)\n",
    "        \n",
    "        # Forward pass: make a prediction for each x event in batch b.\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Get the labels.\n",
    "        label = y_b\n",
    "        y = label.view_as(y_pred)  # reshape label data to the shape of y_pred\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Let's look at the validation set.\n",
    "        \n",
    "        # Torch keeps track of each operation performed on a Tensor, so that it can take the gradient later.\n",
    "        # We don't need to store this information when looking at validation data, so turn it off with\n",
    "        # torch.no_grad().\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Forward pass on validation set.\n",
    "            output = model(val_data)\n",
    "\n",
    "            # Get labels and compute loss again\n",
    "            val_y = val_label.view_as(output)\n",
    "            val_loss = loss_fn(output, val_y)\n",
    "            val_batch_loss.append(val_loss.item())\n",
    "\n",
    "            # Monitor the loss function to prevent overtraining.\n",
    "            if stale_epochs > 20:\n",
    "                break\n",
    "\n",
    "            if val_loss.item() - min_loss < 0:\n",
    "                min_loss = val_loss.item()\n",
    "                stale_epochs = 0\n",
    "                torch.save(model.state_dict(),'pytorch_model_best.pth')\n",
    "            else:\n",
    "                stale_epochs += 1\n",
    "        \n",
    "    losses.append(np.mean(batch_loss))\n",
    "    val_losses.append(np.mean(val_batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with torch.no_grad():\n",
    "    # plot loss vs epoch\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax.plot(losses, label='loss')\n",
    "    ax.plot(val_losses, label='val_loss')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    \n",
    "    # Plot ROC\n",
    "    X_test_in = torch.from_numpy(X_test).float()\n",
    "    Y_predict = model(X_test_in)\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    ax.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.set_xlabel('false positive rate')\n",
    "    ax.set_ylabel('true positive rate')\n",
    "    ax.set_title('receiver operating curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN output vs input variables\n",
    "Here, we will plot the NN output and devision boundary as a function of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regular 2D grid for the inputs\n",
    "myXI, myYI = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))\n",
    "# print shape\n",
    "print(myXI.shape)\n",
    "myZI = model(torch.from_numpy(np.c_[myXI.ravel(), myYI.ravel()]).float())\n",
    "myZI = myZI.reshape(myXI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shoes how to plot the NN output and decision boundary. Does it look optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "# plot contour map of NN output\n",
    "# overlaid with test data points\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "cont_plot = ax.contourf(myXI, myYI, myZI>0.5, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, edgecolors='k')\n",
    "ax.set_xlim(-2,2)\n",
    "ax.set_ylim(-2,2)\n",
    "ax.set_xlabel(VARS[0])\n",
    "ax.set_ylabel(VARS[1])\n",
    "plt.colorbar(cont_plot,ax=ax, boundaries=[0,1],label='NN output')\n",
    "\n",
    "# plot decision boundary\n",
    "# overlaid with test data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** What happens if you increase/decrease the number of hidden layers?\n",
    "\n",
    "**Question 2:** What happens if you increase/decrease the number of nodes per hidden layer?\n",
    "\n",
    "**Question 3:** What happens if you add/remove dropout?\n",
    "\n",
    "**Question 4:** What happens if you add/remove early stopping?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
