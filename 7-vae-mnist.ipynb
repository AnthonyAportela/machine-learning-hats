{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders with Keras and MNIST\n",
    "Author: Charles Kenneth Fisher\n",
    "\n",
    "Adapted from: https://github.com/drckf/mlreview_notebooks/blob/master/jupyter_notebooks/notebooks/NB19_CXVII-Keras_VAE_MNIST.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "The goals of this notebook is to learn how to code a variational autoencoder in Keras. We will discuss hyperparameters, training, and loss-functions. In addition, we will familiarize ourselves with the Keras sequential GUI as well as how to visualize results and make predictions using a VAE with a small number of latent dimensions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook teaches the reader how to build a Variational Autoencoder (VAE) with Keras. The code is a minimally modified, stripped-down version of the code from Lous Tiao in his wonderful [blog post](http://tiao.io/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/) which the reader is strongly encouraged to also read.\n",
    "\n",
    "Our VAE will have Gaussian Latent variables and a Gaussian Posterior distribution $q_\\phi({\\mathbf z}|{\\mathbf x})$ with a diagonal covariance matrix. \n",
    "\n",
    "Recall, that a VAE consists of four essential elements:\n",
    "\n",
    "* A latent variable ${\\mathbf z}$ drawn from a distribution $p({\\mathbf z})$ which in our case will be a Gaussian with mean zero and standard\n",
    "deviation $\\epsilon$.\n",
    "* A decoder $p(\\mathbf{x}|\\mathbf{z})$ that maps latent variables ${\\mathbf z}$ to visible variables ${\\mathbf x}$. In our case, this is just a Multi-Layer Perceptron (MLP) - a neural network with one hidden layer.\n",
    "* An encoder $q_\\phi(\\mathbf{z}|\\mathbf{x})$ that maps examples to the latent space. In our case, this map is just a Gaussian with means and variances that depend on the input: $q_\\phi({\\bf z}|{\\bf x})= \\mathcal{N}({\\bf z}, \\boldsymbol{\\mu}({\\bf x}), \\mathrm{diag}(\\boldsymbol{\\sigma}^2({\\bf x})))$\n",
    "* A cost function consisting of two terms: the reconstruction error and an additional regularization term that minimizes the KL-divergence between the variational and true encoders. Mathematically, the reconstruction error is just the cross-entropy between the samples and their reconstructions. The KL-divergence term can be calculated analytically for this term and can be written as\n",
    "\n",
    "$$-D_{KL}(q_\\phi({\\bf z}|{\\bf x})|p({\\bf z}))={1 \\over 2} \\sum_{j=1}^J \\left (1+\\log{\\sigma_j^2({\\bf x})}-\\mu_j^2({\\bf x}) -\\sigma_j^2({\\bf x})\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VAE](https://lilianweng.github.io/lil-log/assets/images/vae-gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data and specifying hyperparameters\n",
    "\n",
    "In the next section of code, we import the data and specify hyperparameters. The MNIST data are gray scale ranging in values from 0 to 255 for each pixel. We normalize this range to lie between 0 and 1. \n",
    "\n",
    "The hyperparameters we need to specify the architecture and train the VAE are:\n",
    "\n",
    "* The dimension of the hidden layers for encoders and decoders (`intermediate_dim`)\n",
    "* The dimension of the latent space (`latent_dim`)\n",
    "* The standard deviation of latent variables (`epsilon_std`)\n",
    "* Optimization hyper-parameters: `batch_size`, `epochs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import (Input, InputLayer, Dense, Lambda, Layer, \n",
    "                          Add, Multiply)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Load Data and map gray scale 256 to number between zero and 1\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1) / 255.\n",
    "x_test = np.expand_dims(x_test, axis=-1) / 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Find dimensions of input images\n",
    "img_rows, img_cols, img_chns = x_train.shape[1:]\n",
    "\n",
    "# Specify hyperparameters\n",
    "original_dim = img_rows * img_cols\n",
    "intermediate_dim = 256\n",
    "latent_dim = 2\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the loss function\n",
    "\n",
    "Here we specify the loss function. The first block of code is just the reconstruction error which is given by the cross-entropy. The second block of code calculates the KL-divergence analytically and adds it to the loss function with the line `self.add_loss`. It represents the KL-divergence as just another layer in the neural network with the inputs equal to the outputs: the means and variances for the variational encoder (i.e. $\\boldsymbol{\\mu}({\\bf x})$ and $\\boldsymbol{\\sigma}^2({\\bf x})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder\n",
    "\n",
    "The following specifies both the encoder and decoder. The encoder is a MLP with three layers that maps ${\\bf x}$ to $\\boldsymbol{\\mu}({\\bf x})$ and $\\boldsymbol{\\sigma}^2({\\bf x})$, followed by the generation of a latent variable using the reparametrization trick (see main text). The decoder is specified as a single sequential Keras layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "\n",
    "# Reparametrization trick\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(tensor=K.random_normal(shape=(K.shape(x)[0], latent_dim)))\n",
    "\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "# This defines the Encoder which takes noise and input and outputs\n",
    "# the latent variable z\n",
    "encoder = Model(inputs=[x, eps], outputs=z)\n",
    "\n",
    "# Decoder is MLP specified as single Keras Sequential Layer\n",
    "decoder = Sequential([\n",
    "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    Dense(original_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "x_pred = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We now train the model. Even though the loss function is the negative log likelihood (cross-entropy), recall that the KL-layer adds the analytic form of the loss function as well. We also have to reshape the data to make it a vector, and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs=[x, eps], outputs=x_pred, name='vae')\n",
    "vae.compile(optimizer='rmsprop', loss=nll)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "x_test = x_test.reshape(-1, original_dim) / 255.\n",
    "\n",
    "hist = vae.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the loss function\n",
    "\n",
    "We can automatically visualize the loss function as a function of the epoch using the standard Keras interface for fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#for pretty plots\n",
    "golden_size = lambda width: (width, 2. * width / (1 + np.sqrt(5)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=golden_size(6))\n",
    "\n",
    "hist_df = pd.DataFrame(hist.history)\n",
    "hist_df.plot(ax=ax)\n",
    "\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.set_xlabel('# epochs')\n",
    "\n",
    "ax.set_ylim(.99*hist_df[1:].values.min(), \n",
    "            1.1*hist_df[1:].values.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing embedding in latent space\n",
    "\n",
    "Since our latent space is two dimensional, we can think of our encoder as defining a dimensional reduction of the original 784 dimensional space to just two dimensions! We can visualize the structure of this mapping by plotting the MNIST dataset in the latent space, with each point colored by which number it is $[0,1,\\ldots,9]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=golden_size(6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='nipy_spectral')\n",
    "plt.colorbar()\n",
    "plt.savefig('VAE_MNIST_latent.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new examples\n",
    "\n",
    "One of the nice things about VAEs is that they are generative models. Thus, we can generate new examples.\n",
    "\n",
    "* Sampling uniformally in the latent space \n",
    "* Sampling accounting for the fact that the latent space is Gaussian so that we expect most of the data points to be centered around (0,0) and fall off exponentially in all directions. This is done by transforming the uniform grid using the inverse Cumulative Distribution Function (CDF) for the Gaussian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the images\n",
    "n = 5  # figure with 15x15 images\n",
    "quantile_min = 0.01\n",
    "quantile_max = 0.99\n",
    "\n",
    "# Linear Sampling\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "z1_u = np.linspace(5, -5, n)\n",
    "z2_u = np.linspace(5, -5, n)\n",
    "z_grid = np.dstack(np.meshgrid(z1_u, z2_u))\n",
    "\n",
    "x_pred_grid = decoder.predict(z_grid.reshape(n*n, latent_dim)) \\\n",
    "                     .reshape(n, n, img_rows, img_cols)\n",
    "\n",
    "# Plot figure\n",
    "fig, ax = plt.subplots(figsize=golden_size(10))\n",
    "\n",
    "ax.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
    "\n",
    "ax.set_xticks(np.arange(0, n*img_rows, img_rows) + .5 * img_rows)\n",
    "ax.set_xticklabels(map('{:.2f}'.format, z1_u), rotation=90)\n",
    "\n",
    "ax.set_yticks(np.arange(0, n*img_cols, img_cols) + .5 * img_cols)\n",
    "ax.set_yticklabels(map('{:.2f}'.format, z2_u))\n",
    "\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "ax.set_title('Uniform')\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "plt.savefig('VAE_MNIST_fantasy_uniform.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Inverse CDF sampling\n",
    "z1 = norm.ppf(np.linspace(quantile_min, quantile_max, n))\n",
    "z2 = norm.ppf(np.linspace(quantile_max, quantile_min, n))\n",
    "z_grid2 = np.dstack(np.meshgrid(z1, z2))\n",
    "\n",
    "x_pred_grid2 = decoder.predict(z_grid2.reshape(n*n, latent_dim)) \\\n",
    "                     .reshape(n, n, img_rows, img_cols)\n",
    "\n",
    "# Plot figure Inverse CDF sampling\n",
    "fig, ax = plt.subplots(figsize=golden_size(10))\n",
    "\n",
    "ax.imshow(np.block(list(map(list, x_pred_grid2))), cmap='gray')\n",
    "\n",
    "ax.set_xticks(np.arange(0, n*img_rows, img_rows) + .5 * img_rows)\n",
    "ax.set_xticklabels(map('{:.2f}'.format, z1), rotation=90)\n",
    "\n",
    "ax.set_yticks(np.arange(0, n*img_cols, img_cols) + .5 * img_cols)\n",
    "ax.set_yticklabels(map('{:.2f}'.format, z2))\n",
    "\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "ax.set_title('Inverse CDF')\n",
    "ax.grid(False)\n",
    "plt.savefig('VAE_MNIST_fantasy_invCDF.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Play with the standard deviation of the latent variables $\\epsilon$. How does this effect your results?\n",
    "* Generate samples as you increase the number of latent dimensions. Do your generated samples look better? Visualize the latent variables using a dimensional reduction technique such as PCA or t-SNE. How does it compare to the case with two latent dimensions showed above?\n",
    "* Repeat this analysis with the W tagging dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-hats-2021",
   "language": "python",
   "name": "machine-learning-hats-2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nikola": {
   "category": "",
   "date": "2017-07-21 18:38:07 UTC+10:00",
   "description": "",
   "link": "",
   "slug": "variational-inference-with-implicit-approximate-inference-models-wip-pt-8",
   "tags": "",
   "title": "Variational Inference with Implicit Approximate Inference Models (WIP Pt. 8)",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
